{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYBnaHcPRg8w",
        "outputId": "334349b6-1a94-4ef4-d3a6-0593bbab8000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ train Ï†ÄÏû• ÏôÑÎ£å: 16575 samples\n",
            "‚úÖ test Ï†ÄÏû• ÏôÑÎ£å: 430 samples\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "# Config\n",
        "MODEL_NAME = \"dragonkue/BGE-m3-ko\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SAVE_DIR = \"./cached_embeddings\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load SentenceTransformer\n",
        "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "model.max_seq_length = 512\n",
        "\n",
        "# LabelEncoder Í≥µÌÜµÌôî\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def process_split(split_name, csv_path, fit_label=False):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    questions = df[\"Question\"].tolist()\n",
        "    doc_indices = df[\"Doc_index\"].str.replace(\".txt\", \"\", regex=False)\n",
        "\n",
        "    if fit_label:\n",
        "        label_tensor = torch.tensor(label_encoder.fit_transform(doc_indices))\n",
        "        with open(os.path.join(SAVE_DIR, \"label_encoder.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(label_encoder, f)\n",
        "    else:\n",
        "        label_tensor = torch.tensor(label_encoder.transform(doc_indices))\n",
        "\n",
        "    # Embedding (CPU Ï†ÄÏû•!)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.encode(questions, convert_to_tensor=True, device=DEVICE).cpu()\n",
        "\n",
        "    # Ï†ÄÏû•\n",
        "    torch.save(embeddings, os.path.join(SAVE_DIR, f\"{split_name}_emb.pt\"))\n",
        "    torch.save(label_tensor, os.path.join(SAVE_DIR, f\"{split_name}_label.pt\"))\n",
        "    print(f\"{split_name} Ï†ÄÏû• ÏôÑÎ£å: {len(questions)} samples\")\n",
        "\n",
        "# Train + Test Ï≤òÎ¶¨\n",
        "process_split(\"train\", \"./data/csv/train.csv\", fit_label=True)\n",
        "process_split(\"test\", \"./data/csv/test.csv\", fit_label=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi8I6cNxL9VW",
        "outputId": "713a4fd4-b3aa-4f70-f15d-83e60067c4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÅ Training model with seed 42...\n",
            "[Seed 42 | Epoch 1] Loss: 4.5436 | Val Acc: 38.19%\n",
            "[Seed 42 | Epoch 2] Loss: 2.9110 | Val Acc: 53.18%\n",
            "[Seed 42 | Epoch 3] Loss: 2.0993 | Val Acc: 59.85%\n",
            "[Seed 42 | Epoch 4] Loss: 1.7387 | Val Acc: 63.80%\n",
            "[Seed 42 | Epoch 5] Loss: 1.5254 | Val Acc: 65.01%\n",
            "[Seed 42 | Epoch 6] Loss: 1.3815 | Val Acc: 67.33%\n",
            "[Seed 42 | Epoch 7] Loss: 1.2768 | Val Acc: 68.48%\n",
            "[Seed 42 | Epoch 8] Loss: 1.1911 | Val Acc: 69.89%\n",
            "[Seed 42 | Epoch 9] Loss: 1.1390 | Val Acc: 69.26%\n",
            "[Seed 42 | Epoch 10] Loss: 1.0754 | Val Acc: 70.83%\n",
            "[Seed 42 | Epoch 11] Loss: 1.0290 | Val Acc: 71.43%\n",
            "[Seed 42 | Epoch 12] Loss: 0.9754 | Val Acc: 71.89%\n",
            "[Seed 42 | Epoch 13] Loss: 0.9475 | Val Acc: 70.80%\n",
            "[Seed 42 | Epoch 14] Loss: 0.9211 | Val Acc: 72.22%\n",
            "[Seed 42 | Epoch 15] Loss: 0.8914 | Val Acc: 72.16%\n",
            "[Seed 42 | Epoch 16] Loss: 0.8711 | Val Acc: 71.89%\n",
            "[Seed 42 | Epoch 17] Loss: 0.8403 | Val Acc: 72.70%\n",
            "[Seed 42 | Epoch 18] Loss: 0.8199 | Val Acc: 73.12%\n",
            "[Seed 42 | Epoch 19] Loss: 0.8055 | Val Acc: 73.51%\n",
            "[Seed 42 | Epoch 20] Loss: 0.7703 | Val Acc: 73.12%\n",
            "[Seed 42 | Epoch 21] Loss: 0.7704 | Val Acc: 73.91%\n",
            "[Seed 42 | Epoch 22] Loss: 0.7319 | Val Acc: 73.09%\n",
            "[Seed 42 | Epoch 23] Loss: 0.7287 | Val Acc: 73.54%\n",
            "[Seed 42 | Epoch 24] Loss: 0.7095 | Val Acc: 74.48%\n",
            "[Seed 42 | Epoch 25] Loss: 0.6977 | Val Acc: 74.15%\n",
            "[Seed 42 | Epoch 26] Loss: 0.6874 | Val Acc: 74.51%\n",
            "[Seed 42 | Epoch 27] Loss: 0.6929 | Val Acc: 74.18%\n",
            "[Seed 42 | Epoch 28] Loss: 0.6629 | Val Acc: 74.48%\n",
            "[Seed 42 | Epoch 29] Loss: 0.6483 | Val Acc: 74.54%\n",
            "[Seed 42 | Epoch 30] Loss: 0.6376 | Val Acc: 74.03%\n",
            "[Seed 42 | Epoch 31] Loss: 0.6293 | Val Acc: 74.45%\n",
            "[Seed 42 | Epoch 32] Loss: 0.6201 | Val Acc: 74.30%\n",
            "[Seed 42 | Epoch 33] Loss: 0.6036 | Val Acc: 74.69%\n",
            "[Seed 42 | Epoch 34] Loss: 0.6006 | Val Acc: 75.44%\n",
            "[Seed 42 | Epoch 35] Loss: 0.6043 | Val Acc: 75.11%\n",
            "[Seed 42 | Epoch 36] Loss: 0.5848 | Val Acc: 74.93%\n",
            "[Seed 42 | Epoch 37] Loss: 0.5840 | Val Acc: 74.90%\n",
            "[Seed 42 | Epoch 38] Loss: 0.5655 | Val Acc: 74.42%\n",
            "[Seed 42 | Epoch 39] Loss: 0.5558 | Val Acc: 74.90%\n",
            "[Seed 42 | Epoch 40] Loss: 0.5451 | Val Acc: 74.96%\n",
            "‚úÖ Saved model with seed 42 ‚Üí ./mlp_seed42.pt (Best Val Acc: 75.44%)\n",
            "\n",
            "üîÅ Training model with seed 123...\n",
            "[Seed 123 | Epoch 1] Loss: 4.5464 | Val Acc: 37.95%\n",
            "[Seed 123 | Epoch 2] Loss: 2.9115 | Val Acc: 50.92%\n",
            "[Seed 123 | Epoch 3] Loss: 2.1074 | Val Acc: 57.38%\n",
            "[Seed 123 | Epoch 4] Loss: 1.7502 | Val Acc: 61.96%\n",
            "[Seed 123 | Epoch 5] Loss: 1.5172 | Val Acc: 64.37%\n",
            "[Seed 123 | Epoch 6] Loss: 1.3900 | Val Acc: 66.03%\n",
            "[Seed 123 | Epoch 7] Loss: 1.2775 | Val Acc: 67.93%\n",
            "[Seed 123 | Epoch 8] Loss: 1.1979 | Val Acc: 68.78%\n",
            "[Seed 123 | Epoch 9] Loss: 1.1375 | Val Acc: 69.59%\n",
            "[Seed 123 | Epoch 10] Loss: 1.0629 | Val Acc: 69.02%\n",
            "[Seed 123 | Epoch 11] Loss: 1.0267 | Val Acc: 69.44%\n",
            "[Seed 123 | Epoch 12] Loss: 0.9808 | Val Acc: 70.86%\n",
            "[Seed 123 | Epoch 13] Loss: 0.9488 | Val Acc: 70.56%\n",
            "[Seed 123 | Epoch 14] Loss: 0.9186 | Val Acc: 70.92%\n",
            "[Seed 123 | Epoch 15] Loss: 0.8838 | Val Acc: 71.46%\n",
            "[Seed 123 | Epoch 16] Loss: 0.8551 | Val Acc: 72.82%\n",
            "[Seed 123 | Epoch 17] Loss: 0.8612 | Val Acc: 72.19%\n",
            "[Seed 123 | Epoch 18] Loss: 0.8303 | Val Acc: 72.85%\n",
            "[Seed 123 | Epoch 19] Loss: 0.7911 | Val Acc: 72.67%\n",
            "[Seed 123 | Epoch 20] Loss: 0.7774 | Val Acc: 72.13%\n",
            "[Seed 123 | Epoch 21] Loss: 0.7504 | Val Acc: 72.88%\n",
            "[Seed 123 | Epoch 22] Loss: 0.7334 | Val Acc: 72.97%\n",
            "[Seed 123 | Epoch 23] Loss: 0.7301 | Val Acc: 73.39%\n",
            "[Seed 123 | Epoch 24] Loss: 0.7027 | Val Acc: 73.27%\n",
            "[Seed 123 | Epoch 25] Loss: 0.7011 | Val Acc: 73.60%\n",
            "[Seed 123 | Epoch 26] Loss: 0.6715 | Val Acc: 73.57%\n",
            "[Seed 123 | Epoch 27] Loss: 0.6708 | Val Acc: 73.70%\n",
            "[Seed 123 | Epoch 28] Loss: 0.6578 | Val Acc: 73.63%\n",
            "[Seed 123 | Epoch 29] Loss: 0.6408 | Val Acc: 73.88%\n",
            "[Seed 123 | Epoch 30] Loss: 0.6366 | Val Acc: 73.88%\n",
            "[Seed 123 | Epoch 31] Loss: 0.6321 | Val Acc: 73.91%\n",
            "[Seed 123 | Epoch 32] Loss: 0.6135 | Val Acc: 73.76%\n",
            "[Seed 123 | Epoch 33] Loss: 0.6014 | Val Acc: 74.45%\n",
            "[Seed 123 | Epoch 34] Loss: 0.5928 | Val Acc: 73.67%\n",
            "[Seed 123 | Epoch 35] Loss: 0.5814 | Val Acc: 73.57%\n",
            "[Seed 123 | Epoch 36] Loss: 0.5744 | Val Acc: 74.30%\n",
            "[Seed 123 | Epoch 37] Loss: 0.5697 | Val Acc: 74.18%\n",
            "[Seed 123 | Epoch 38] Loss: 0.5573 | Val Acc: 74.57%\n",
            "[Seed 123 | Epoch 39] Loss: 0.5524 | Val Acc: 74.51%\n",
            "[Seed 123 | Epoch 40] Loss: 0.5516 | Val Acc: 74.57%\n",
            "‚úÖ Saved model with seed 123 ‚Üí ./mlp_seed123.pt (Best Val Acc: 74.57%)\n",
            "\n",
            "üîÅ Training model with seed 2025...\n",
            "[Seed 2025 | Epoch 1] Loss: 4.5442 | Val Acc: 39.34%\n",
            "[Seed 2025 | Epoch 2] Loss: 2.9008 | Val Acc: 53.12%\n",
            "[Seed 2025 | Epoch 3] Loss: 2.1071 | Val Acc: 59.58%\n",
            "[Seed 2025 | Epoch 4] Loss: 1.7381 | Val Acc: 62.93%\n",
            "[Seed 2025 | Epoch 5] Loss: 1.5283 | Val Acc: 64.65%\n",
            "[Seed 2025 | Epoch 6] Loss: 1.3931 | Val Acc: 66.52%\n",
            "[Seed 2025 | Epoch 7] Loss: 1.2661 | Val Acc: 67.66%\n",
            "[Seed 2025 | Epoch 8] Loss: 1.2055 | Val Acc: 68.66%\n",
            "[Seed 2025 | Epoch 9] Loss: 1.1453 | Val Acc: 70.44%\n",
            "[Seed 2025 | Epoch 10] Loss: 1.0776 | Val Acc: 71.22%\n",
            "[Seed 2025 | Epoch 11] Loss: 1.0416 | Val Acc: 71.07%\n",
            "[Seed 2025 | Epoch 12] Loss: 1.0050 | Val Acc: 71.34%\n",
            "[Seed 2025 | Epoch 13] Loss: 0.9565 | Val Acc: 71.58%\n",
            "[Seed 2025 | Epoch 14] Loss: 0.9173 | Val Acc: 72.43%\n",
            "[Seed 2025 | Epoch 15] Loss: 0.9021 | Val Acc: 71.83%\n",
            "[Seed 2025 | Epoch 16] Loss: 0.8582 | Val Acc: 72.16%\n",
            "[Seed 2025 | Epoch 17] Loss: 0.8386 | Val Acc: 72.34%\n",
            "[Seed 2025 | Epoch 18] Loss: 0.8143 | Val Acc: 72.58%\n",
            "[Seed 2025 | Epoch 19] Loss: 0.8105 | Val Acc: 73.09%\n",
            "[Seed 2025 | Epoch 20] Loss: 0.7712 | Val Acc: 73.30%\n",
            "[Seed 2025 | Epoch 21] Loss: 0.7371 | Val Acc: 73.82%\n",
            "[Seed 2025 | Epoch 22] Loss: 0.7464 | Val Acc: 73.24%\n",
            "[Seed 2025 | Epoch 23] Loss: 0.7274 | Val Acc: 73.51%\n",
            "[Seed 2025 | Epoch 24] Loss: 0.7071 | Val Acc: 73.76%\n",
            "[Seed 2025 | Epoch 25] Loss: 0.7005 | Val Acc: 74.18%\n",
            "[Seed 2025 | Epoch 26] Loss: 0.6811 | Val Acc: 74.06%\n",
            "[Seed 2025 | Epoch 27] Loss: 0.6738 | Val Acc: 74.27%\n",
            "[Seed 2025 | Epoch 28] Loss: 0.6525 | Val Acc: 73.76%\n",
            "[Seed 2025 | Epoch 29] Loss: 0.6448 | Val Acc: 74.12%\n",
            "[Seed 2025 | Epoch 30] Loss: 0.6431 | Val Acc: 74.63%\n",
            "[Seed 2025 | Epoch 31] Loss: 0.6302 | Val Acc: 74.18%\n",
            "[Seed 2025 | Epoch 32] Loss: 0.6245 | Val Acc: 74.15%\n",
            "[Seed 2025 | Epoch 33] Loss: 0.6097 | Val Acc: 74.39%\n",
            "[Seed 2025 | Epoch 34] Loss: 0.5884 | Val Acc: 75.02%\n",
            "[Seed 2025 | Epoch 35] Loss: 0.6032 | Val Acc: 74.36%\n",
            "[Seed 2025 | Epoch 36] Loss: 0.5674 | Val Acc: 74.90%\n",
            "[Seed 2025 | Epoch 37] Loss: 0.5831 | Val Acc: 74.63%\n",
            "[Seed 2025 | Epoch 38] Loss: 0.5692 | Val Acc: 74.48%\n",
            "[Seed 2025 | Epoch 39] Loss: 0.5537 | Val Acc: 74.72%\n",
            "[Seed 2025 | Epoch 40] Loss: 0.5511 | Val Acc: 74.93%\n",
            "‚úÖ Saved model with seed 2025 ‚Üí ./mlp_seed2025.pt (Best Val Acc: 75.02%)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Config\n",
        "CACHE_DIR = \"./cached_embeddings\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEEDS = [42, 123, 2025]\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Load cached embeddings and labels\n",
        "X = torch.load(os.path.join(CACHE_DIR, \"train_emb.pt\"))\n",
        "y = torch.load(os.path.join(CACHE_DIR, \"train_label.pt\"))\n",
        "with open(os.path.join(CACHE_DIR, \"label_encoder.pkl\"), \"rb\") as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "# Dataset class\n",
        "class QDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# MLP Ï†ïÏùò\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# üîÅ Ïó¨Îü¨ seedÎ°ú Î∞òÎ≥µ ÌïôÏäµ\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\nüîÅ Training model with seed {seed}...\")\n",
        "\n",
        "    # ÏãúÎìú Í≥†Ï†ï\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Train/Validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(QDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(QDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "    mlp = MLP(X.shape[1], len(le.classes_)).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(EPOCHS):\n",
        "        # ----- TRAIN -----\n",
        "        mlp.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = mlp(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # ----- VALIDATION -----\n",
        "        mlp.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                logits = mlp(xb)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        val_acc = correct / total\n",
        "        best_acc = max(best_acc, val_acc)\n",
        "\n",
        "        print(f\"[Seed {seed} | Epoch {epoch+1}] Loss: {avg_train_loss:.4f} | Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "    # Î™®Îç∏ Ï†ÄÏû•\n",
        "    save_path = f\"./mlp_seed{seed}.pt\"\n",
        "    torch.save(mlp.state_dict(), save_path)\n",
        "    print(f\"Saved model with seed {seed} ‚Üí {save_path} (Best Val Acc: {best_acc:.2%})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oTzIw4SMMei",
        "outputId": "af1f5b17-26cf-4e73-87a0-d275fe2bcf30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ensemble Recall@3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 712.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Ensemble Recall@3: 412/430 = 95.81%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Config\n",
        "CACHE_DIR = \"./cached_embeddings\"\n",
        "MODEL_PATHS = [\"mlp_seed42.pt\", \"mlp_seed123.pt\", \"mlp_seed2025.pt\"]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load data\n",
        "X_test = torch.load(f\"{CACHE_DIR}/test_emb.pt\")\n",
        "y_test = torch.load(f\"{CACHE_DIR}/test_label.pt\")\n",
        "\n",
        "# Load label encoder (for inverse_transform if needed)\n",
        "with open(f\"{CACHE_DIR}/label_encoder.pkl\", \"rb\") as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "# Model architecture (ÎèôÏùºÌïú MLP Íµ¨Ï°∞ Ïç®Ïïº Ìï®)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Load models\n",
        "models = []\n",
        "for path in MODEL_PATHS:\n",
        "    model = MLP(X_test.shape[1], len(le.classes_))\n",
        "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "# Ensemble inference\n",
        "recall_hits = 0\n",
        "total = len(y_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, total, 16), desc=\"Ensemble Recall@3\"):\n",
        "        xb = X_test[i:i+16].to(DEVICE)\n",
        "        logits_sum = torch.zeros((xb.size(0), len(le.classes_)), device=DEVICE)\n",
        "\n",
        "        for model in models:\n",
        "            logits_sum += model(xb)\n",
        "\n",
        "        top3 = torch.topk(logits_sum, k=3, dim=1).indices.cpu()\n",
        "        labels = y_test[i:i+16]\n",
        "\n",
        "        for true, pred_top3 in zip(labels, top3):\n",
        "            if true.item() in pred_top3.tolist():\n",
        "                recall_hits += 1\n",
        "\n",
        "print(f\"\\nEnsemble Recall@3: {recall_hits}/{total} = {recall_hits / total:.2%}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
